{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.min_rows\", 50)\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import joblib \n",
    "import json\n",
    "import time\n",
    "from tableone import TableOne\n",
    "from scipy import sparse\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "data_dir ='./data'\n",
    "basedir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import clinprediction.omop_fx\n",
    "importlib.reload(clinprediction.omop_fx)\n",
    "\n",
    "import clinprediction.patient_fx\n",
    "importlib.reload(clinprediction.patient_fx)\n",
    "\n",
    "import clinprediction.match_fx\n",
    "importlib.reload(clinprediction.match_fx)\n",
    "\n",
    "import clinprediction.model_fx\n",
    "importlib.reload(clinprediction.match_fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_options = None # ['testdate','AD','control'] \n",
    "in_dir = basedir + 'data/'\n",
    "pdir = basedir + 'cohort_selection/'\n",
    "output_dir = basedir + 'cohort_selection_out/'\n",
    "\n",
    "if load_options is not None:\n",
    "    options = load_in_options(*load_options)\n",
    "    odir_main = options['odir_main']\n",
    "    demo_cols = options['demo_cols']\n",
    "    cols_match_num = options['match_params']['cols_match_num']\n",
    "    cols_match_vis = options['match_params']['cols_match_vis']\n",
    "    cols_match_cat = options['match_params']['cols_match_cat']\n",
    "else: \n",
    "    demo_cols = ['Sex','date_age'] # demographic columns to use in model\n",
    "    cols_match_num = ['year_of_birth', 'min_date_age', 'date_age', 'yrs_in_ehr'] # numerical columns to use in matching\n",
    "    cols_match_vis = ['log_n_prev_visits','log_n_concepts','log_neg_earliestday'] # visit related columns to use in matching\n",
    "    cols_match_cat = ['Sex', 'RaceEthnicity'] # categorical column to use in matching\n",
    "\n",
    "    options = {\n",
    "    'ndate': 'testdate',        # date identifier\n",
    "    'input_dir': in_dir,      # input directory\n",
    "    'output_dir': output_dir, # output directory\n",
    "    'ratio': 8,               # ratio for control matching\n",
    "    'timefilt_range': [-365*7, -365*5, -365*3, -365*1, -1],\n",
    "    'dxgroup': 'AD',\n",
    "    'comparison':'control',\n",
    "    'match_cohorts': True,\n",
    "    'match_params': {'cols_match_cat': cols_match_cat, 'cols_match_num': cols_match_num, \n",
    "                     'cols_match_vis':cols_match_vis,\n",
    "                     'match_scale_features':False, 'match_with_replacement':False,\n",
    "                     'match_with_pscore': True},\n",
    "    'random_state': 500, 'random_seed': 500,\n",
    "    'cv': 5,  \n",
    "    }\n",
    "\n",
    "    odir_main = output_dir + '{}_{}v{}/'.format(options['ndate'], options['dxgroup'], options['comparison'])\n",
    "    options['odir_main'] = odir_main\n",
    "    os.makedirs(odir_main, exist_ok = True)\n",
    "\n",
    "    # Save options\n",
    "    with open(odir_main + 'options.json', 'w') as fp:\n",
    "        json.dump(options, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clinspokeprediction.omop_fx import OMOPData\n",
    "\n",
    "start = time.time()\n",
    "omopdata = OMOPData(omopdir = 'data/omop/')\n",
    "\n",
    "# Read in OMOP information for patients of interest.\n",
    "try: \n",
    "    omopdata.load_compressed(pdir)\n",
    "except: \n",
    "    omopdata.read_in_omop_csv(directory = pdir, read_in_controls = True)\n",
    "    omopdata.save_compressed()\n",
    "    \n",
    "# Concept look-up dictionary\n",
    "conceptdict = omopdata.concepts.set_index('concept_id')['concept_name'].to_dict()\n",
    "\n",
    "print(omopdata.size_of_data_col('iscontrol'))\n",
    "print('Finished reading in OMOP data. took {} minutes'.format((time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in patients, basic filtering\n",
    "from clinspokeprediction.patient_fx import read_in_patients, age_visit_timefilt, filter_pts\n",
    "from clinspokeprediction.match_fx import match_patients\n",
    "\n",
    "timefilt_min = np.array(options['timefilt_range']).min()\n",
    "\n",
    "if 'allpts_timefiltmin_file' in options:\n",
    "    allpts = pd.read_csv(options['allpts_timefiltmin_file'])\n",
    "else: \n",
    "    # read in patients\n",
    "    cohortpts, controlpts, all_visits = read_in_patients(options)\n",
    "    print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts.shape, controlpts.shape))\n",
    "\n",
    "    # cohort mindatept: first AD, dementia, or cognitive drug. \n",
    "    cohortpts, controlpts = filter_pts(cohortpts, controlpts)\n",
    "    print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts.shape, controlpts.shape))\n",
    "    controlpts[options['dxgroup']]=0 # 0 for the label\n",
    "\n",
    "    # filter by index 0 age\n",
    "    index0age_thresh = 55\n",
    "    print('remove if index 0 age is less than {} years'.format(index0age_thresh))\n",
    "    print('remove {} cases'.format((cohortpts.mindatept_age >= index0age_thresh).sum()))\n",
    "    cohortpts = cohortpts[cohortpts.mindatept_age >= index0age_thresh]\n",
    "    print('remove {} controls'.format((controlpts.mindatept_age >= index0age_thresh).sum()))\n",
    "    controlpts = controlpts[controlpts.mindatept_age >= index0age_thresh]\n",
    "\n",
    "    print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts.shape, controlpts.shape))\n",
    "\n",
    "    cohort_dxf = 1-cohortpts.set_index('person_id')['dxf']\\\n",
    "        .apply(lambda x: pd.Series(x.split(','))).unstack().dropna()\\\n",
    "        .reset_index().pivot(index = 'person_id', columns = 0, values = 0)\\\n",
    "        .isna().astype(int)\n",
    "\n",
    "    # look at cohort\n",
    "    TableOne(cohortpts[cohortpts[dxgroup]==1]\\\n",
    "                 .merge(cohort_dxf[\\\n",
    "                                   np.setdiff1d(cohort_dxf.columns,cohortpts.columns)].reset_index(), \n",
    "                                     on = 'person_id', how = 'left', validate = '1:1'),\n",
    "             columns = ['Sex','RaceEthnicity','n_visits','e_age','mindatept_age',\n",
    "                        '90old','lds_birthyear'] + list(cohort_dxf.columns))\n",
    "\n",
    "    # filter patient based upon minimum time point for model\n",
    "    timefilt_min = np.array(options['timefilt_range']).min()\n",
    "\n",
    "    # get information for patients using earliest timepoint of interest \n",
    "    print('earliest timepoint: ', timefilt_min)\n",
    "    cohortpts, controlpts = age_visit_timefilt(cohortpts, controlpts, all_visits, timefilt_min)\n",
    "\n",
    "    # Use patients with 'timefilt_min' visits for prediction model\n",
    "    cohortpts = cohortpts[cohortpts['date_age'] >= cohortpts['min_date_age']]\n",
    "    controlpts = controlpts[controlpts['date_age'] >= controlpts['min_date_age']]\n",
    "    print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts.shape, controlpts.shape))\n",
    "    allpts = cohortpts.append(controlpts)\n",
    "\n",
    "    allpts.to_csv(odir_main + 'allpts_timefiltmin.csv', index = False)\n",
    "    options['allpts_timefiltmin_file'] = odir_main + 'allpts_timefiltmin.csv'\n",
    "    save_updated_options(options)\n",
    "\n",
    "display_table(allpts.reset_index(), groupby = 'AD', options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "tosplit_person_id = allpts['person_id'].to_numpy() # all person_id\n",
    "tosplit_dxgroup = allpts[options['dxgroup']].to_numpy() # all labels\n",
    "\n",
    "if 'person_id_train_all_file' not in options:\n",
    "    person_id_train, person_id_test, _, _ = \\\n",
    "    train_test_split(tosplit_person_id, tosplit_dxgroup, stratify = tosplit_dxgroup, \n",
    "                     test_size = .3, random_state = options['random_state']) \n",
    "\n",
    "    person_id_train = np.sort(person_id_train)\n",
    "    person_id_test = np.sort(person_id_test)\n",
    "\n",
    "    options['person_id_train_all_file'] = odir_main + 'train_personid_all.npy'\n",
    "    np.save(options['person_id_train_all_file'], person_id_train)\n",
    "    \n",
    "    options['person_id_test_all_file'] = odir_main + 'test_personid_all.npy'\n",
    "    np.save(options['person_id_test_all_file'], person_id_test)\n",
    "else:\n",
    "    person_id_train = np.load(options['person_id_train_all_file'])\n",
    "    person_id_test = np.load(options['person_id_test_all_file'])\n",
    "    \n",
    "print('train number pid:', len(person_id_train))\n",
    "print('test number pid:', len(person_id_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for each time prior to AD onset\n",
    "\n",
    "For each time point prior to onset, we preprocess the data and run a random forest model.\\\n",
    "For interpretation purposes, we then match our training cohort and run another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clinspokeprediction.omop_fx import filter_omopdata_by_time\n",
    "save_long = True\n",
    "\n",
    "for timefilt in options['timefilt_range']:\n",
    "    odir_tf = options['odir_main'] + str(timefilt) + '/'\n",
    "    os.makedirs(odir_tf, exist_ok = True)\n",
    "\n",
    "    if os.path.isfile(odir_tf + 'omop_count_demo_visit.joblib'):\n",
    "        print(odir_tf + 'omop_count_demo_visit.joblib exists.')\n",
    "        print('getting information at timefilt:{}'.format(timefilt))\n",
    "        omop_pt_tf_input = joblib.load(odir_tf + 'omop_count_demo_visit.joblib')\n",
    "        allpt_tf = omop_pt_tf_input['allpt_tf']\n",
    "        allpt_tf_train = omop_pt_tf_input['allpt_tf_train'] \n",
    "        allpt_tf_test = omop_pt_tf_input['allpt_tf_test']\n",
    "        pts_train_tf = omop_pt_tf_input['train_pts']\n",
    "        pts_test_tf = omop_pt_tf_input['test_pts']\n",
    "        feat_concepts = omop_pt_tf_input['feat_concepts']\n",
    "\n",
    "        allptomop = pd.read_csv(odir_tf + 'patient_sentence_long.csv')\n",
    "        del omop_pt_tf_input\n",
    "    else: \n",
    "        print('getting information at timefilt:{}'.format(timefilt))\n",
    "        cohortpts_tf, controlpts_tf = age_visit_timefilt(cohortpts, controlpts, all_visits, timefilt)\n",
    "        pts_tf = cohortpts_tf.append(controlpts_tf)\n",
    "\n",
    "        if options['demo_cols'] is not None:\n",
    "            demo_data = pts_tf.reset_index()[['person_id']+ options['demo_cols']]\n",
    "            demo_data = pd.get_dummies(demo_data).set_index('person_id')\n",
    "        else: demo_data = None\n",
    "\n",
    "        pts_train_tf = np.intersect1d(pts_tf.person_id, person_id_train)\n",
    "        pts_test_tf = np.intersect1d(pts_tf.person_id, person_id_test)\n",
    "\n",
    "        print('train number pid: ',len(pts_train_tf))\n",
    "        print('test number pid: ', len(pts_test_tf))\n",
    "\n",
    "        print('extracting OMOP information: conditions, drugs, measures... ')\n",
    "        (conditions, drugs, measures) = filter_omopdata_by_time(omopdata, pts_tf, timefilt = timefilt)\n",
    "        measures_ab = process_abnormal_measures(measures)\n",
    "\n",
    "        # turn into long format\n",
    "        allptomop = drugs.rename({'drug_concept_id':'concept_id'},axis=1)\\\n",
    "            .append(conditions.rename({'condition_concept_id':'concept_id'},axis=1))\\\n",
    "            .append(measures_ab.rename({'measurement_concept_id':'concept_id'},axis=1))\\\n",
    "            [['person_id','concept_id','datediff']]\n",
    "\n",
    "        # add domain\n",
    "        allptomop = allptomop.merge(omopdata.concepts[['concept_id','domain_id']], \n",
    "                                    on = 'concept_id', how = 'left')\n",
    "\n",
    "        # feature space\n",
    "        feat_concepts = np.sort(allptomop.concept_id.unique())\n",
    "\n",
    "        # now that the concepts are filtered, count number of concepts, number of concepts per domain, and earliest \"date\" for an entry\n",
    "        allpt_omopcount = allptomop.groupby('person_id')['concept_id'].nunique().to_frame('n_concepts')\\\n",
    "            .merge(allptomop.groupby('person_id')['datediff'].min().dt.days.rename('earliest_day'), \n",
    "               left_index = True, right_index = True, how = 'outer')\\\n",
    "            .merge(allptomop.groupby('person_id')['domain_id'].value_counts().unstack().fillna(0), \n",
    "               left_index = True, right_index = True, how = 'outer')\n",
    "\n",
    "        # combine patient info with omop and demographics\n",
    "        allpt_tf = pts_tf[['person_id','min_date_age','date_age', \n",
    "                      'yrs_in_ehr','n_prev_visits', 'Sex',\n",
    "                      'RaceEthnicity', 'year_of_birth', \n",
    "                      dxgroup]].set_index('person_id').sort_index()\\\n",
    "                    .merge(allpt_omopcount, left_index = True, # combine with counts\n",
    "                           right_index = True, how = 'left')\\\n",
    "                    .merge(demo_data, left_index = True, # combine with demographics\n",
    "                           right_index = True, suffixes = ('','_'), how = 'left')\n",
    "\n",
    "        # preprocess again\n",
    "        allpt_tf.loc[allpt_tf.yrs_in_ehr > 50, 'yrs_in_ehr'] = 50\n",
    "        allpt_tf.loc[:,'log_n_prev_visits'] = allpt_tf.n_prev_visits.apply(lambda x: np.log(x+.1))\n",
    "        allpt_tf.loc[:,'log_n_concepts'] = allpt_tf.n_concepts.apply(lambda x: 0 if pd.isna(x) else np.log(x+.1))\n",
    "        allpt_tf.loc[:,'log_neg_earliestday'] = allpt_tf.earliest_day.fillna(.1)\\\n",
    "                        .apply(lambda x: 0 if pd.isna(x) else np.log(-x))\n",
    "\n",
    "        print('AD after computing timepoint info...{}'.format(timefilt))\n",
    "        display(pts_tf.AD.value_counts(dropna = False)) # after computing timepoint info\n",
    "\n",
    "        print()\n",
    "        print('AD after merging with omop info...{}'.format(timefilt))\n",
    "        display(allpt_tf.AD.value_counts(dropna = False)) # after merging with omop info\n",
    "\n",
    "        print('get train/test')\n",
    "        # get info on all patients \n",
    "        allpt_tf_train = allpt_tf.loc[pts_train_tf] # TAKE TRAINING PATIENTS\n",
    "        allpt_tf_test = allpt_tf.loc[pts_test_tf] # TAKE TESTING PATIENTS\n",
    "\n",
    "        print()\n",
    "        print('AD in train{}'.format(timefilt))\n",
    "        display(allpt_tf_train.AD.value_counts(dropna = False))\n",
    "\n",
    "        print()\n",
    "        print('AD in test{}'.format(timefilt))\n",
    "        display(allpt_tf_test.AD.value_counts(dropna = False))\n",
    "\n",
    "        if save_long:\n",
    "            allptomop.merge(allpt_tf[['min_date_age','date_age','yrs_in_ehr',\n",
    "                    'n_prev_visits','Sex_Female','RaceEthnicity','AD']],\n",
    "                    left_on = 'person_id', right_index = True, how = 'left')\\\n",
    "                    .to_csv(odir_tf + 'patient_sentence_long.csv', index = False)\n",
    "\n",
    "        joblib.dump({'allpt_tf':allpt_tf, 'allpt_tf_train': allpt_tf_train, \n",
    "                 'allpt_tf_test':allpt_tf_test,\n",
    "                 'train_pts':pts_train_tf, 'test_pts':pts_test_tf,\n",
    "                  'feat_concepts':feat_concepts},\n",
    "                  odir_tf + 'omop_count_demo_visit.joblib')\n",
    "        \n",
    "            #### INITIAL MODELS WITHOUT MATCHING\n",
    "\n",
    "    if timefilt != timefilt_min:\n",
    "        if 'train_personid_mintimefilt_file' not in options:\n",
    "            options['train_personid_mintimefilt_file'] = odir_main + 'train_personid_mintimefilt.npy'\n",
    "            options['test_personid_mintimefilt_file'] = odir_main + 'test_personid_mintimefilt.npy'\n",
    "            save_updated_options(options)\n",
    "        train_personid = np.load(options['train_personid_mintimefilt_file'])\n",
    "        test_personid = np.load(options['test_personid_mintimefilt_file'])\n",
    "\n",
    "        print('train:',pts_train_tf.shape, train_personid.shape)\n",
    "        print('test:',pts_test_tf.shape, test_personid.shape)\n",
    "    else:\n",
    "        train_personid = np.load(options['train_personid_mintimefilt_file'])\n",
    "        test_personid = np.load(options['test_personid_mintimefilt_file'])\n",
    "\n",
    "\n",
    "    # Training data preparation\n",
    "    if os.path.isfile(odir_tf + 'model_unmatched_input_data.joblib'):\n",
    "        print('loading in saved model inputs... ')\n",
    "        unmatched_model_inputs = joblib.load(odir_tf + 'model_unmatched_input_data.joblib')\n",
    "\n",
    "        X_train = unmatched_model_inputs['X_train']\n",
    "        X_test = unmatched_model_inputs['X_test']\n",
    "        feature_names = unmatched_model_inputs['feature_names']\n",
    "        varthresh = unmatched_model_inputs['varthresh']\n",
    "        y_train = unmatched_model_inputs['y_train']\n",
    "        y_test = unmatched_model_inputs['y_test']\n",
    "\n",
    "        print('variance thresholded n features:', varthresh.get_support(1).shape)\n",
    "        feature_names_var = feature_names[varthresh.get_support(1)]\n",
    "        feature_name_info = feature_names.to_frame('concept_id').rename_axis('')\\\n",
    "            .merge(omopdata.concepts.groupby('concept_id').head(1), \n",
    "                   on = 'concept_id', how = 'left').set_index('concept_id')\n",
    "        print('X_train shape: {}, X_test shape: {}'.format(X_train.shape, X_test.shape))\n",
    "        print('length of y_train: {}. \\n\\tsum of y_train: {}. \\n\\tmean of y_train: {:0.07f}'.format(\\\n",
    "                        len(y_train), y_train.sum(), y_train.mean()))\n",
    "        print('length of y_test: {}. \\n\\tsum of y_test: {}. \\n\\tmean of y_test: {:0.07f}'.format(\\\n",
    "                    len(y_test), y_test.sum(), y_test.mean()))\n",
    "    else: \n",
    "        print('preparing X_train')\n",
    "        allptomop_pivot_train = pivot_omop(allptomop, pts_train_tf)\n",
    "        if 'train_personid_mintimefilt_file' in options:\n",
    "            X_train = allptomop_pivot_train.loc[train_personid]\n",
    "            print(X_train.shape)\n",
    "        else: \n",
    "            X_train = allptomop_pivot_train\n",
    "            #X_train = X_train[X_train.sum(axis=1)>=4]\n",
    "            print('X_train: ', X_train.shape)\n",
    "            train_personid = X_train.index\n",
    "\n",
    "        feature_names = X_train.columns\n",
    "\n",
    "        # remove features with 0 variance\n",
    "        varthresh = VarianceThreshold().fit(X_train)\n",
    "        print('variance thresholded n features:', varthresh.get_support(1).shape)\n",
    "\n",
    "        feature_names_var = feature_names[varthresh.get_support(1)]\n",
    "        feature_name_info = feature_names.to_frame('concept_id').rename_axis('')\\\n",
    "            .merge(omopdata.concepts.groupby('concept_id').head(1), \n",
    "                   on = 'concept_id', how = 'left').set_index('concept_id')\n",
    "    \n",
    "        # Test data prep\n",
    "        print('preparing X_test')\n",
    "        allptomop_pivot_test = pivot_omop(allptomop, pts_test_tf, feature_names_var)\n",
    "        if 'test_personid_mintimefilt_file' in options:\n",
    "            X_test = allptomop_pivot_test.loc[test_personid, feature_names_var]\n",
    "            print(X_test.shape)\n",
    "        else:\n",
    "            X_test = allptomop_pivot_test \n",
    "            #X_test = X_test[X_test.sum(axis=1)>=4]\n",
    "            test_personid = X_test.index\n",
    "            print('X_test: ', X_test.shape)\n",
    "\n",
    "        if timefilt == options['timefilt_range'][0]:\n",
    "            print('Save patients with concepts if timefilt_min')\n",
    "            options['train_personid_mintimefilt_file'] = odir_main + 'train_personid_mintimefilt.npy'\n",
    "            options['test_personid_mintimefilt_file'] = odir_main + 'test_personid_mintimefilt.npy'\n",
    "            np.save(options['train_personid_mintimefilt_file'], train_personid)\n",
    "            np.save(options['test_personid_mintimefilt_file'], test_personid)\n",
    "            save_updated_options(options)\n",
    "\n",
    "        print('load y_train and y_test')\n",
    "        y_train = allpt_tf.loc[train_personid][dxgroup].to_numpy()\n",
    "        print('length of y_train: {}. \\n\\tsum of y_train: {}. \\n\\tmean of y_train: {:0.07f}'.format(\\\n",
    "                        len(y_train), y_train.sum(), y_train.mean()))\n",
    "        y_test = allpt_tf.loc[test_personid][dxgroup].to_numpy()\n",
    "        print('length of y_test: {}. \\n\\tsum of y_test: {}. \\n\\tmean of y_test: {:0.07f}'.format(\\\n",
    "                    len(y_test), y_test.sum(), y_test.mean()))\n",
    "\n",
    "        print('save.')\n",
    "        joblib.dump({'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test, \n",
    "                 'feature_names':feature_names,'varthresh':varthresh},\n",
    "                  odir_tf + 'model_unmatched_input_data.joblib')\n",
    "    \n",
    "    allpt_tf['visits_per_yr'] = allpt_tf['n_prev_visits']/(allpt_tf['yrs_in_ehr']+.1)\n",
    "    # demographics of updated pts? \n",
    "    print('train:',pts_train_tf.shape, train_personid.shape)\n",
    "    print('test:',pts_test_tf.shape, test_personid.shape)\n",
    "    display_table(allpt_tf.loc[np.concatenate((train_personid,test_personid))].reset_index(),  groupby = 'AD', options = options)\n",
    "    N_FEATURES = varthresh.get_support().sum()\n",
    "    X_train2 = varthresh.transform(X_train)\n",
    "    X_train_sparse = sparse.csr_matrix(X_train2)\n",
    "    X_test2 = X_test.to_numpy()\n",
    "\n",
    "    ### RANDOM FOREST MODEL\n",
    "    fname = odir_tf+'rf_unmatched_model.joblib'\n",
    "    if os.path.isfile(fname):\n",
    "        print(fname, 'exists, loading in... ')\n",
    "        rf_unmatched_dict = joblib.load(fname)\n",
    "        rf_feat_import = feature_context(rf_unmatched_dict['feat_import'], feature_name_info, \n",
    "                         import_col = 'rf_import', modelkind = 'rf_unmatched')\n",
    "    else: \n",
    "        np.random.seed(1100)\n",
    "        pt_choice = np.concatenate((np.where(y_train)[0], \n",
    "                        np.random.choice(np.where(1-y_train)[0], int(y_train.sum())*ratio)))\n",
    "\n",
    "        rf_unmatched_dict = rf_model(X_train2[pt_choice], y_train[pt_choice],\n",
    "                     X_test2, y_test, feature_names_var, options, odir_tf = odir_tf, modelsuffix = '_unmatched',\n",
    "                     n_its = 300)\n",
    "\n",
    "        rf_feat_import_unmatched = feature_context(rf_unmatched_dict['feat_import'], feature_name_info, \n",
    "                            import_col = 'rf_import', modelkind = 'rf_unmatched', odir_tf = odir_tf)\n",
    "        \n",
    "    joblib.dump(rf_unmatched_dict, fname)\n",
    "    \n",
    "    \n",
    "    ## PREPROCESS MATCHED PATIENTS\n",
    "    cols_match_num = options['match_params']['cols_match_num'] + options['match_params']['cols_match_vis']\n",
    "    cols_match_cat = options['match_params']['cols_match_cat']\n",
    "    \n",
    "    # Training data prep\n",
    "    if os.path.isfile(odir_tf + 'cohort_control_pt_train_tf.joblib'):\n",
    "        print('loading in saved model inputs... ')\n",
    "        loaded_matched_pts = joblib.load(odir_tf + 'cohort_control_pt_train_tf.joblib')\n",
    "\n",
    "        cohortpts_tf_train = loaded_matched_pts['cohortpts_tf_train']\n",
    "        controlpts_tf_train = loaded_matched_pts['controlpts_tf_train']\n",
    "        allpt_tf_matched = loaded_matched_pts['allpt_tf_matched']\n",
    "\n",
    "        del loaded_matched_pts\n",
    "    else: \n",
    "        cohortpts_tf_train = allpt_tf_train[allpt_tf_train[dxgroup]==1].reset_index()\n",
    "        controlpts_tf_train = allpt_tf_train[allpt_tf_train[dxgroup]==0].reset_index()\n",
    "        print('cohortpts shape (prior): {}\\ncontrolpts shape: {}'.format(cohortpts_tf_train.shape, \n",
    "                                                                 controlpts_tf_train.shape))\n",
    "\n",
    "        cohortpts_tf_train = cohortpts_tf_train[cohortpts_tf_train.person_id.isin(train_personid)]\n",
    "        controlpts_tf_train = controlpts_tf_train[controlpts_tf_train.person_id.isin(train_personid)]\n",
    "        print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts_tf_train.shape, \n",
    "                                                                 controlpts_tf_train.shape))\n",
    "\n",
    "        # match patients\n",
    "        cohortpts_tf_train, controlpts_tf_train, _ = \\\n",
    "                match_patients(cohortpts_tf_train, controlpts_tf_train, dxgroup, \n",
    "                            cols_match_cat = cols_match_cat, \n",
    "                            cols_match_num = cols_match_num, \n",
    "                           ratio = options['ratio'], return_split = True)\n",
    "\n",
    "        allpt_tf_matched = cohortpts_tf_train.append(controlpts_tf_train).set_index('person_id')\n",
    "        joblib.dump({'cohortpts_tf_train': cohortpts_tf_train, 'controlpts_tf_train': controlpts_tf_train, \n",
    "                      'allpt_tf_matched': allpt_tf_matched}, odir_tf + 'cohort_control_pt_train_tf.joblib')\n",
    "\n",
    "        # plot table\n",
    "        mytable = TableOne(allpt_tf_matched, columns = cols_match_num + cols_match_cat, \n",
    "                    groupby=dxgroup, categorical = cols_match_cat, smd = True, \n",
    "                       pval = True);\n",
    "        display(mytable)\n",
    "        mytable.to_csv(odir_tf + 'allpt_train_tf_matched.csv')\n",
    "            \n",
    "    # Test data prep\n",
    "    if os.path.isfile(odir_tf + 'cohort_control_pt_test_tf.joblib'):\n",
    "        print('loading in saved model inputs... ')\n",
    "        loaded_matched_pts = joblib.load(odir_tf + 'cohort_control_pt_test_tf.joblib')\n",
    "\n",
    "        cohortpts_tf_test = loaded_matched_pts['cohortpts_tf_test']\n",
    "        controlpts_tf_test = loaded_matched_pts['controlpts_tf_test']\n",
    "        allpt_tf_test_matched = loaded_matched_pts['allpt_tf_test_matched']\n",
    "\n",
    "        del loaded_matched_pts\n",
    "    else: \n",
    "        # match test patients\n",
    "        cohortpts_tf_test = allpt_tf_test[allpt_tf_test[dxgroup]==1].reset_index()\n",
    "        controlpts_tf_test = allpt_tf_test[allpt_tf_test[dxgroup]==0].reset_index()\n",
    "        print('cohortpts shape (prior): {}\\ncontrolpts shape: {}'.format(cohortpts_tf_test.shape, controlpts_tf_test.shape))\n",
    "        cohortpts_tf_test = cohortpts_tf_test[cohortpts_tf_test.person_id.isin(test_personid)]\n",
    "        controlpts_tf_test = controlpts_tf_test[controlpts_tf_test.person_id.isin(test_personid)]\n",
    "        print('cohortpts shape: {}\\ncontrolpts shape: {}'.format(cohortpts_tf_test.shape, controlpts_tf_test.shape))\n",
    "\n",
    "        # match patients\n",
    "        cohortpts_tf_test, controlpts_tf_test, _ = match_patients(cohortpts_tf_test, controlpts_tf_test, dxgroup, \n",
    "                            cols_match_cat = cols_match_cat, cols_match_num = cols_match_num, \n",
    "                           ratio = options['ratio'], return_split = True)\n",
    "        allpt_tf_test_matched = cohortpts_tf_test.append(controlpts_tf_test).set_index('person_id')\n",
    "\n",
    "        joblib.dump({'cohortpts_tf_test': cohortpts_tf_test, 'controlpts_tf_test': controlpts_tf_test, \n",
    "                 'allpt_tf_test_matched': allpt_tf_test_matched},\n",
    "                 odir_tf + 'cohort_control_pt_test_tf.joblib')\n",
    "\n",
    "        # plot table\n",
    "        mytable = TableOne(allpt_tf_test_matched, columns = cols_match_num + cols_match_cat, \n",
    "                    groupby=dxgroup, categorical = cols_match_cat, smd = True, \n",
    "                       pval = True);\n",
    "        display(mytable)\n",
    "        mytable.to_csv(odir_tf + 'allpt_test_tf_matched.csv')\n",
    "        \n",
    "    try: allptomop_pivot_train\n",
    "    except: allptomop_pivot_train = pivot_omop(allptomop, pts_train_tf)\n",
    "    try: allptomop_pivot_test\n",
    "    except: allptomop_pivot_test = pivot_omop(allptomop, pts_test_tf, feature_names_var)\n",
    "    X_train = allptomop_pivot_train.loc[allpt_tf_matched.index]\n",
    "    y_train = allpt_tf_matched.loc[allpt_tf_matched.index][dxgroup].to_numpy()\n",
    "\n",
    "    print('matched X_train.shape ',X_train.shape)\n",
    "    print('matched y_train.shape ', len(y_train))\n",
    "    print('matched train:',pts_train_tf.shape, train_personid.shape, allpt_tf_matched.shape, X_train.shape)\n",
    "    feature_names = X_train.columns\n",
    "    train_personid_matched = allpt_tf_matched.index\n",
    "    feature_name_info = feature_names.to_frame('concept_id').rename_axis('')\\\n",
    "        .merge(omopdata.concepts.groupby('concept_id').head(1), on = 'concept_id', how = 'left')\\\n",
    "        .set_index('concept_id')\n",
    "    \n",
    "    varthresh = VarianceThreshold().fit(X_train)\n",
    "    N_FEATURES = varthresh.get_support().sum()\n",
    "    print('varthresh n features: ', varthresh.get_support(1).shape)\n",
    "\n",
    "    feature_names_var = feature_names[varthresh.get_support(1)]\n",
    "    feature_name_info = feature_names.to_frame('concept_id').rename_axis('')\\\n",
    "        .merge(omopdata.concepts.groupby('concept_id').head(1), on = 'concept_id', how = 'left').set_index('concept_id')\n",
    "    X_test = allptomop_pivot_test.loc[test_personid, # options['test_personid_mintimefilt'], \n",
    "                                  feature_names_var].fillna(0)\n",
    "    y_test = allpt_tf.loc[test_personid][dxgroup].to_numpy()\n",
    "    print('X_test.shape ', X_test.shape)\n",
    "    X_train2 = varthresh.transform(X_train)\n",
    "    X_test2 = X_test.to_numpy()\n",
    "    X_train_sparse = sparse.csr_matrix(X_train2)\n",
    "\n",
    "    ## MATCHED COHORT RANDOM FOREST MODELS\n",
    "    fname = odir_tf+'rf_matched_model.joblib'\n",
    "    if os.path.isfile(fname):\n",
    "        print(fname, 'exists, loading in... ')\n",
    "        rf_matched_dict = joblib.load(fname)\n",
    "        rf_feat_import = feature_context(rf_matched_dict['feat_import'], feature_name_info, \n",
    "                         import_col = 'rf_import', modelkind = 'rf_matched')\n",
    "    else: \n",
    "        rf_matched_dict = rf_model(X_train2, y_train,\n",
    "                 X_test2, y_test, feature_names_var, options, \n",
    "                 odir_tf = odir_tf, modelsuffix = '_matched', \n",
    "                 n_its = 300) \n",
    "\n",
    "        rf_feat_import = feature_context(rf_matched_dict['feat_import'], feature_name_info, \n",
    "                import_col = 'rf_import', modelkind = 'rf_matched', odir_tf = odir_tf)\n",
    "        \n",
    "        ## updating features\n",
    "        feature_context(rf_matched_dict['feat_import'], feature_name_info, \n",
    "                             import_col = 'rf_import', modelkind = 'rf_matched', odir_tf = odir_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehrml",
   "language": "python",
   "name": "ehrml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
